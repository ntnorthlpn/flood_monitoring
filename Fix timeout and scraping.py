#!/usr/bin/env python3
"""
‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Timeout ‡πÅ‡∏•‡∏∞ Web Scraping
‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö
"""

# ======================================================================
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏û‡∏¥‡πà‡∏° Retry Logic ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö API Timeout
# ======================================================================

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time

def create_retry_session(
    retries=3,
    backoff_factor=0.3,
    status_forcelist=(500, 502, 504),
    session=None
):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á requests session ‡∏ó‡∏µ‡πà‡∏°‡∏µ retry logic
    
    Args:
        retries: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà
        backoff_factor: ‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏≠‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏≠‡∏á (0.3, 0.6, 1.2 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
        status_forcelist: HTTP status codes ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ retry
        session: existing session (optional)
    
    Returns:
        requests.Session: Session ‡∏û‡∏£‡πâ‡∏≠‡∏° retry logic
    """
    session = session or requests.Session()
    retry = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
        status_forcelist=status_forcelist,
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session


def get_thaiwater_data_with_retry(station_code, agency_code, max_attempts=2):
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å ThaiWater API ‡∏û‡∏£‡πâ‡∏≠‡∏° retry logic
    
    Args:
        station_code: ‡∏£‡∏´‡∏±‡∏™‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ
        agency_code: ‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô
        max_attempts: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏•‡∏≠‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    
    Returns:
        dict: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥ ‡∏´‡∏£‡∏∑‡∏≠ None
    """
    THAIWATER_API_BASE = "https://api.thaiwater.net/v1"
    THAIWATER_API_KEY = None  # ‡πÉ‡∏™‡πà API key ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    
    url = f"{THAIWATER_API_BASE}/WaterlevelObservation"
    params = {
        "latest": "true",
        "agencyCode": agency_code,
        "stationCode": station_code
    }
    
    headers = {"Accept": "application/json"}
    if THAIWATER_API_KEY:
        headers["Authorization"] = f"Bearer {THAIWATER_API_KEY}"
    
    for attempt in range(1, max_attempts + 1):
        try:
            print(f"   üîÑ Attempt {attempt}/{max_attempts}...")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á session ‡∏û‡∏£‡πâ‡∏≠‡∏° retry
            session = create_retry_session()
            
            # ‡∏•‡∏î timeout ‡πÄ‡∏õ‡πá‡∏ô 15 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡πÅ‡∏ó‡∏ô 30
            response = session.get(
                url,
                params=params,
                headers=headers,
                timeout=15
            )
            
            if response.status_code == 404:
                print(f"   ‚ö†Ô∏è Station not found (404)")
                return None
            elif response.status_code == 401:
                print(f"   ‚ö†Ô∏è Unauthorized (401)")
                return None
            
            response.raise_for_status()
            print(f"   ‚úÖ Success on attempt {attempt}")
            return response.json()
            
        except requests.exceptions.Timeout:
            print(f"   ‚è±Ô∏è Timeout on attempt {attempt}")
            if attempt < max_attempts:
                wait_time = 2 * attempt  # ‡∏£‡∏≠ 2, 4 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
                print(f"   ‚è≥ Waiting {wait_time}s before retry...")
                time.sleep(wait_time)
            else:
                print(f"   ‚ùå All attempts failed")
                return None
                
        except requests.exceptions.RequestException as e:
            print(f"   ‚ùå Error on attempt {attempt}: {e}")
            if attempt < max_attempts:
                time.sleep(2)
            else:
                return None
        
        finally:
            if 'session' in locals():
                session.close()
    
    return None


# ======================================================================
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ó‡∏µ‡πà 2: ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Web Scraping ‡πÉ‡∏´‡πâ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
# ======================================================================

from bs4 import BeautifulSoup
import re
import json

def get_chiangmai_thaiwater_data_improved(station_id=None):
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö Chiang Mai ThaiWater ‡πÅ‡∏ö‡∏ö‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô
    ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    """
    url = "https://chiangmai.thaiwater.net/wl"
    
    try:
        print(f"   üåê Fetching from {url}...")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'th-TH,th;q=0.9,en;q=0.8',
        }
        
        # ‡∏•‡∏î timeout ‡πÄ‡∏õ‡πá‡∏ô 20 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug
        with open('chiangmai_thaiwater_debug.html', 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f"   üíæ Saved HTML to chiangmai_thaiwater_debug.html")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ======================================================================
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏´‡∏≤ element ‡∏ó‡∏µ‡πà‡∏°‡∏µ text "P.1" ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        # ======================================================================
        print(f"   üîç Method 1: Searching for text 'P.1'...")
        station_elements = soup.find_all(string=re.compile(r'P\.\d+'))
        
        if station_elements:
            print(f"   ‚úÖ Found {len(station_elements)} station code(s)")
            for elem in station_elements[:3]:
                print(f"      - {elem.strip()}")
                parent = elem.parent
                # ‡∏´‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
                if parent:
                    nearby_numbers = re.findall(r'\d+\.?\d*', parent.get_text())
                    if nearby_numbers:
                        print(f"        Numbers nearby: {nearby_numbers}")
        
        # ======================================================================
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏´‡∏≤ JSON data ‡πÉ‡∏ô window variable
        # ======================================================================
        print(f"   üîç Method 2: Looking for JavaScript data...")
        scripts = soup.find_all('script')
        
        for script in scripts:
            if script.string and ('var ' in script.string or 'let ' in script.string):
                # ‡∏´‡∏≤ JSON arrays ‡∏´‡∏£‡∏∑‡∏≠ objects
                matches = re.finditer(
                    r'(?:var|let|const)\s+(\w+)\s*=\s*(\[[\s\S]*?\]|\{[\s\S]*?\});',
                    script.string
                )
                
                for match in matches:
                    var_name = match.group(1)
                    var_value = match.group(2)
                    
                    # ‡∏•‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô JSON
                    try:
                        data = json.loads(var_value)
                        
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                        if isinstance(data, list) and len(data) > 0:
                            first_item = data[0]
                            if isinstance(first_item, dict):
                                # ‡∏•‡∏≠‡∏á print keys ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á
                                print(f"   ‚úÖ Found variable '{var_name}' with {len(data)} items")
                                print(f"      Keys: {list(first_item.keys())[:10]}")
                                
                                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ station code ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                                for key in first_item.keys():
                                    if 'station' in key.lower() or 'code' in key.lower():
                                        print(f"      Possible station field: {key} = {first_item[key]}")
                                
                                # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥
                                for key in first_item.keys():
                                    if any(w in key.lower() for w in ['level', 'water', 'depth', '‡∏£‡∏∞‡∏î‡∏±‡∏ö']):
                                        print(f"      Possible water level field: {key} = {first_item[key]}")
                                
                                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å JSON ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π
                                with open('chiangmai_data.json', 'w', encoding='utf-8') as f:
                                    json.dump(data, f, indent=2, ensure_ascii=False)
                                print(f"      üíæ Saved to chiangmai_data.json")
                                
                    except json.JSONDecodeError:
                        continue
        
        # ======================================================================
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: ‡∏´‡∏≤‡∏à‡∏≤‡∏Å API endpoint ‡∏ó‡∏µ‡πà‡∏ã‡πà‡∏≠‡∏ô‡∏≠‡∏¢‡∏π‡πà
        # ======================================================================
        print(f"   üîç Method 3: Looking for API endpoints...")
        
        # ‡∏´‡∏≤ URL ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô API
        for script in scripts:
            if script.string:
                api_urls = re.findall(
                    r'["\']([^"\']*(?:api|data)[^"\']*\.(?:json|php|aspx))["\']',
                    script.string
                )
                if api_urls:
                    print(f"   üì° Found potential API URLs:")
                    for api_url in set(api_urls):
                        print(f"      - {api_url}")
        
        # ======================================================================
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 4: ‡∏´‡∏≤‡∏à‡∏≤‡∏Å meta tags ‡∏´‡∏£‡∏∑‡∏≠ data attributes
        # ======================================================================
        print(f"   üîç Method 4: Checking data attributes...")
        
        elements_with_data = soup.find_all(attrs={"data-station": True})
        if elements_with_data:
            print(f"   ‚úÖ Found {len(elements_with_data)} elements with data-station")
            for elem in elements_with_data[:3]:
                print(f"      {elem.name}: {elem.attrs}")
        
        elements_with_data = soup.find_all(attrs={"data-level": True})
        if elements_with_data:
            print(f"   ‚úÖ Found {len(elements_with_data)} elements with data-level")
            for elem in elements_with_data[:3]:
                print(f"      {elem.name}: {elem.attrs}")
        
        print(f"   üí° Check chiangmai_thaiwater_debug.html and chiangmai_data.json for more details")
        
        return None
        
    except requests.exceptions.Timeout:
        print(f"   ‚è±Ô∏è Website timeout after 20 seconds")
        return None
    except requests.exceptions.RequestException as e:
        print(f"   ‚ùå Error fetching website: {e}")
        return None
    except Exception as e:
        print(f"   ‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return None


# ======================================================================
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ó‡∏µ‡πà 3: ‡∏õ‡∏£‡∏±‡∏ö Message ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# ======================================================================

def create_summary_message_improved(location, analysis, thaiwater_info=None, website_info=None):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á
    """
    message_lines = [
        f"üåä <b>‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ô‡πâ‡∏≥‡πÅ‡∏°‡πà‡∏ô‡πâ‡∏≥‡∏õ‡∏¥‡∏á</b>",
        "",
        f"üìç <b>‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà:</b> {location['name']}",
        ""
    ]
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á
    data_sources = []
    
    if website_info:
        data_sources.append("‚úÖ ‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå ‡∏à.‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà")
        message_lines.extend([
            "<b>üåê ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå:</b>",
            f"  üíß ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥: {website_info.get('water_level', 'N/A')} ‡∏°.(‡∏£‡∏ó‡∏Å.)",
            ""
        ])
    else:
        data_sources.append("‚ö†Ô∏è ‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå ‡∏à.‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà (‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ)")
    
    if thaiwater_info:
        data_sources.append("‚úÖ ThaiWater API")
        message_lines.extend([
            "<b>üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å ThaiWater API:</b>",
            f"  üíß ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥: {thaiwater_info.get('water_level', 'N/A')} ‡∏°.(‡∏£‡∏ó‡∏Å.)",
            ""
        ])
    else:
        data_sources.append("‚ö†Ô∏è ThaiWater API (Timeout/‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)")
    
    if analysis:
        data_sources.append("‚úÖ Open-Meteo ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå")
        message_lines.extend([
            f"<b>üîÆ ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå (Open-Meteo):</b>",
            f"  üíß ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏ô‡πâ‡∏≥‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: {analysis['current_discharge']:.1f} m¬≥/s",
            f"  üìä ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞: {analysis['current_emoji']} {analysis['current_text']}",
            ""
        ])
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    message_lines.extend([
        "<b>üì° ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:</b>"
    ])
    for source in data_sources:
        message_lines.append(f"  {source}")
    
    message_lines.extend([
        "",
        f"üïê <i>‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï: {datetime.now().strftime('%d/%m/%Y %H:%M')} ‡∏ô.</i>",
        "",
        "<i>üí° ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏≤‡∏á‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á</i>"
    ])
    
    return "\n".join(message_lines)


# ======================================================================
# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
# ======================================================================

if __name__ == "__main__":
    print("=" * 70)
    print("üß™ Testing Improved Functions")
    print("=" * 70)
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Web Scraping
    print("\n1Ô∏è‚É£ Testing Web Scraping...")
    get_chiangmai_thaiwater_data_improved("P.1")
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö API with Retry
    print("\n2Ô∏è‚É£ Testing ThaiWater API with Retry...")
    result = get_thaiwater_data_with_retry("P.1", "G07003", max_attempts=2)
    if result:
        print("‚úÖ API call successful")
    else:
        print("‚ö†Ô∏è API call failed after retries")
    
    print("\n" + "=" * 70)
    print("‚úÖ Test complete")
    print("=" * 70)
